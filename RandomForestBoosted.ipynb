{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import exposure\n",
    "#from skimage.segmentation import quickshift, slic, watershed\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from osgeo import ogr\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import earthpy as py\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from xgboost import XGBRFClassifier\n",
    "#import xgboost as xgb\n",
    "\n",
    "# import packages for hyperparameters tuning\n",
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPOSITE\n",
    "\n",
    "naip_fn = r'C:\\Users\\10333076966\\Documents\\GEOPROCESSAMENTO\\Temp\\SUBSET\\COMPOSITE.tif'\n",
    " \n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    "nbands = naip_ds.RasterCount\n",
    "band_data = []\n",
    "\n",
    "for i in range(1, nbands+1):\n",
    "    band = naip_ds.GetRasterBand(i).ReadAsArray()\n",
    "    #band_norm = preprocessing.normalize(band)\n",
    "\n",
    "    #out_put = ((band-band.min())/ (band.max()-band.min()))\n",
    "\n",
    "    #scaler = MinMaxScaler()\n",
    "    #bands = scaler.fit_transform(band)\n",
    "    band_data.append(band)\n",
    "    \n",
    "    #band_data.append(out_put)\n",
    "\n",
    "    #band_data.append(band)\n",
    "\n",
    "band_data = np.dstack(band_data)\n",
    "p2, p98 = np.percentile(band_data, (2,98))\n",
    "\n",
    "img = exposure.rescale_intensity(band_data, in_range=(p2, p98))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGMENTO\n",
    "naip_seg = r'C:\\Users\\10333076966\\Documents\\GEOPROCESSAMENTO\\Temp\\SUBSET\\SEGMENTACAO.tif'\n",
    "\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_seg)\n",
    "segments = naip_ds.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features(segment_pixels):\n",
    "    SD = []\n",
    "    features = []\n",
    "    npixels, nbands = segment_pixels.shape\n",
    "    for b in range(nbands):\n",
    "        stats = scipy.stats.describe(band_data[:, i])\n",
    "        SD.append(np.std(band_data[:, i]))\n",
    "        band_stats = list(stats)[1:]\n",
    "        if npixels == 1:\n",
    "            # in this case the variance = nan, change it 0.0\n",
    "            band_stats[3] = 0.0\n",
    "        features += band_stats\n",
    "        \n",
    "    features.append(np.array(SD))\n",
    "    return features\n",
    "\n",
    "def segment_sizes(segment_pixels):\n",
    "    area = segment_pixels.shape[0]*segment_pixels.shape[1]*30*30\n",
    "    length = (segment_pixels.shape[0]*segment_pixels.shape[1]+segment_pixels.shape[0]*segment_pixels.shape[1])*30\n",
    "    elongation = np.max(segment_pixels.shape)/np.min(segment_pixels.shape)\n",
    "    compactness = np.sqrt(4*area)/length\n",
    "    size_stats = [area,length,elongation,compactness]\n",
    "    return size_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids = np.unique(segments)\n",
    "objects = []\n",
    "object_ids = []\n",
    "for id in segment_ids:\n",
    "    segment_pixels = img[segments == id]\n",
    "    object_features = segment_features(segment_pixels)\n",
    "    object_features_2 = segment_sizes(segment_pixels)\n",
    "    objects.append(object_features)\n",
    "    objects.append(object_features_2)\n",
    "    object_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_features_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read shapefile to geopandas geodataframe\n",
    "gdf = gpd.read_file(r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\2001_L7\\Amostras_2001\\teste_amostras_2001.shp')\n",
    "# get names of land cover classes/labels\n",
    "class_names = gdf['label'].unique()\n",
    "# create a unique id (integer) for each land cover class/label\n",
    "class_ids = np.arange(class_names.size) + 1\n",
    "# create a pandas data frame of the labels and ids and save to csv\n",
    "df = pd.DataFrame({'label': class_names, 'id': class_ids})\n",
    "#df.to_csv(r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\1991_L5\\Amostras_1991\\class_lookup.csv')\n",
    "# add a new column to geodatafame with the id for each class/label\n",
    "gdf['id'] = gdf['label'].map(dict(zip(class_names, class_ids)))\n",
    " \n",
    "# split the truth data into training and test data sets and save each to a new shapefile\n",
    "gdf_train = gdf.sample(frac=0.8)  # 80% of observations assigned to training data (30% to test data)\n",
    "gdf_test = gdf.drop(gdf_train.index)\n",
    "# save training and test data to shapefiles\n",
    "gdf_train.to_file(r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\2001_L7\\Amostras_2001\\train_data.shp')\n",
    "gdf_test.to_file(r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\2001_L7\\Amostras_2001\\test_data.shp')\n",
    "print(f'os labels ser√£o: \\n {df}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\2001_L7\\Amostras_2001\\train_data.shp'\n",
    "train_ds = ogr.Open(train_fn)\n",
    "lyr = train_ds.GetLayer()\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "# retrieve the rasterized data and print basic stats\n",
    "data = target_ds.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterized observation (truth, training and test) data\n",
    "ground_truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# get unique values (0 is the background, or no data, value so it is not included) for each land cover type\n",
    "\n",
    "classes = np.unique(ground_truth)[1:]\n",
    "\n",
    "# for each class (land cover type) record the associated segment IDs\n",
    "segments_per_class = {}\n",
    "for klass in classes:\n",
    "    segments_of_class = segments[ground_truth == klass]\n",
    "    segments_per_class[klass] = set(segments_of_class)\n",
    " \n",
    "# make sure no segment ID represents more than one class\n",
    "# intersection = set()\n",
    "# accum = set()\n",
    "# for class_segments in segments_per_class.values():\n",
    "#     intersection |= accum.intersection(class_segments)\n",
    "#     accum |= class_segments\n",
    "# assert len(intersection) == 0, \"Segment(s) represent multiple classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.copy(segments)\n",
    "threshold = train_img.max() + 1  # make the threshold value greater than any land cover class value\n",
    "\n",
    "# all pixels in training segments assigned value greater than threshold\n",
    "for klass in classes:\n",
    "    class_label = threshold + klass\n",
    "    for segment_id in segments_per_class[klass]:\n",
    "        train_img[train_img == segment_id] = class_label\n",
    " \n",
    "# training segments receive land cover class value, all other segments 0\n",
    "train_img[train_img <= threshold] = 0\n",
    "train_img[train_img > threshold] -= threshold\n",
    "\n",
    "\n",
    "# create objects and labels for training data\n",
    "training_objects = []\n",
    "training_labels = []\n",
    "for klass in classes:\n",
    "    class_train_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class[klass]]\n",
    "    training_labels += [klass] * len(class_train_object)\n",
    "    training_objects += class_train_object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "\n",
    "test_fn = r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\2001_L7\\Amostras_2001\\test_data.shp'\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_test = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_test.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_test.SetProjection(naip_ds.GetProjection())\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_test, [1], lyr, options=options)\n",
    "# retrieve the rasterized data and print basic stats\n",
    "\n",
    "\n",
    "# rasterized observation (truth, training and test) data\n",
    "ground_truth_test = target_test.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# get unique values (0 is the background, or no data, value so it is not included) for each land cover type\n",
    "classes_test = np.unique(ground_truth_test)[1:]\n",
    "\n",
    "# for each class (land cover type) record the associated segment IDs\n",
    "segments_per_class_test = {}\n",
    "for klass_test in classes_test:\n",
    "    segments_of_class_test = segments[ground_truth_test == klass_test]\n",
    "    segments_per_class_test[klass_test] = set(segments_of_class_test)\n",
    " \n",
    "\n",
    "test_image = np.copy(segments)\n",
    "threshold = test_image.max() + 1  # make the threshold value greater than any land cover class value\n",
    "\n",
    "# all pixels in training segments assigned value greater than threshold\n",
    "for klass_test in classes_test:\n",
    "    class_label = threshold + klass_test\n",
    "    for segment_id in segments_per_class_test[klass_test]:\n",
    "        test_image[test_image == segment_id] = class_label\n",
    " \n",
    "# training segments receive land cover class value, all other segments 0\n",
    "test_image[test_image <= threshold] = 0\n",
    "test_image[test_image > threshold] -= threshold\n",
    "\n",
    "\n",
    "# create objects and labels for TESTING data\n",
    "testing_objects = []\n",
    "testing_labels = []\n",
    "for klass_test in classes:\n",
    "    class_test_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class_test[klass_test]]\n",
    "    testing_labels += [klass_test] * len(class_test_object)\n",
    "    testing_objects += class_test_object\n",
    " \n",
    "\n",
    "testing_labels = np.asarray(testing_labels, dtype=np.int32)\n",
    "training_labels = np.asarray(training_labels, dtype=np.int32)\n",
    "\n",
    "for i in range(len(training_labels)):\n",
    "    if training_labels[i] == 6:\n",
    "        training_labels[i] = 0\n",
    "\n",
    "\n",
    "for i in range(len(testing_labels)):\n",
    "    if testing_labels[i] == 6:\n",
    "        testing_labels[i] = 0\n",
    "\n",
    "\n",
    "print(f'o maximo de treino labels √©: {np.max(training_labels)}' )\n",
    "print(f'o minimo de treino labels √©: {np.min(training_labels)}' )\n",
    "\n",
    "\n",
    "print(f'o maximo de teste labels √©: {np.max(testing_labels)}' )\n",
    "print(f'o minimo de teste labels √©: {np.min(testing_labels)}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Padr√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #random forest\n",
    "# classifier = RandomForestClassifier(n_jobs=-1)  # setup random forest classifier\n",
    "# classifier.fit(training_objects, training_labels)  # fit rf classifier\n",
    "# predicted = classifier.predict(objects)  # predict with rf classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "# y_pred = classifier.predict(testing_objects)\n",
    "# cm = confusion_matrix(testing_labels, y_pred)\n",
    "# print(cm)\n",
    "# accuracy_score(testing_labels,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = cross_val_score(estimator=classifier,X=testing_objects,y=testing_labels,cv=10)\n",
    "# print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDSEARCH CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# parameters = [{'n_estimators':[10,20,30,40,50,60,70],\n",
    "#                'max_depth': [3,4,5,7],\n",
    "#                'criterion':['entropy'],\n",
    "#                'min_samples_split':[5,4,6,7,8],\n",
    "#                'max_features':['auto', 'sqrt', 'log2']\n",
    "#                }]\n",
    "# grid_search = GridSearchCV(estimator= classifier,\n",
    "#                            param_grid= parameters,\n",
    "#                            scoring = 'accuracy',\n",
    "#                            n_jobs = -1,\n",
    "#                            verbose=2,\n",
    "#                            cv = 10)\n",
    "\n",
    "# grid_search.fit(training_objects,training_labels)\n",
    "# best_accuracy = grid_search.best_score_\n",
    "# best_parameters = grid_search.best_params_\n",
    "# print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "# print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'criterion' : hp.choice('criterion', ['entropy','gini']),\n",
    "         'max_depth' : hp.quniform('max_depth',10,1200,10),\n",
    "         'max_features' : hp.choice('max_features',['auto','sqrt','log2',None]),\n",
    "         'min_samples_leaf' : hp.uniform('min_samples_leaf',0,0.5),\n",
    "         'min_samples_split' : hp.uniform('min_samples_split',0,1),\n",
    "         'n_estimators': hp.choice('n_estimators',[10,50,350,550,750,1200,1300,1500])\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier( criterion = space['criterion'],\n",
    "                                    max_depth = int(space['max_depth']),\n",
    "                                    max_features = space['max_features'],\n",
    "                                    min_samples_leaf = space['min_samples_leaf'],\n",
    "                                    min_samples_split = space['min_samples_split'],\n",
    "                                    n_estimators = space['n_estimators']\n",
    "    )\n",
    "    \n",
    "    accuracy = cross_val_score(model, training_objects,training_labels,cv=10).mean()\n",
    "    return {'loss' : -accuracy , 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space=space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals=80,\n",
    "            trials=trials\n",
    "           )\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = {0: 'entropy', 1: 'gini'}\n",
    "feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\n",
    "est = {0: 10, 1: 50, 2: 350, 3: 550, 4: 750, 5: 1200, 6: 1300, 7: 1500}\n",
    "\n",
    "print(crit[best['criterion']])\n",
    "print(feat[best['max_features']])\n",
    "print(est[best['n_estimators']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = int(best['max_depth']), \n",
    "                                       max_features = feat[best['max_features']], \n",
    "                                       min_samples_leaf = best['min_samples_leaf'], \n",
    "                                       min_samples_split = best['min_samples_split'], \n",
    "                                       n_estimators = est[best['n_estimators']]).fit(training_objects,training_labels)\n",
    "\n",
    "                                       \n",
    "predictionforest = trainedforest.predict(testing_objects)\n",
    "print(confusion_matrix(testing_labels,predictionforest))\n",
    "print(f'A acur√°cia global √© de : {round(accuracy_score(testing_labels,predictionforest),4)}')\n",
    "print(classification_report(testing_labels,predictionforest))\n",
    "acc5 = accuracy_score(testing_labels,predictionforest)\n",
    "kappa = cohen_kappa_score(testing_labels, predictionforest)\n",
    "print(f'O kappa √© de : {round(kappa,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = trainedforest.predict(objects)\n",
    "# create numpy array from rf classifiation and save to raster\n",
    "clf = np.copy(segments)\n",
    "\n",
    "for segment_id, klass in zip(segment_ids, predicted):\n",
    "    clf[clf == segment_id] = klass\n",
    " \n",
    "mask = np.sum(img, axis=2)  # this section masks no data values\n",
    "mask[mask > 0.0] = 1.0\n",
    "mask[mask == 0.0] = 1.0\n",
    "#mask[mask == 0.0] = -1.0\n",
    "clf = np.multiply(clf, mask)\n",
    "clf[clf < 0] = -9999.0\n",
    " \n",
    " \n",
    "clfds = driverTiff.Create(r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\2001_L7\\CLASSIFIED_SNIC_2001_.tif', naip_ds.RasterXSize, naip_ds.RasterYSize,\n",
    "                          1, gdal.GDT_Float32)  # this section saves to raster\n",
    "clfds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "clfds.SetProjection(naip_ds.GetProjection())\n",
    "clfds.GetRasterBand(1).SetNoDataValue(-9999.0)\n",
    "clfds.GetRasterBand(1).WriteArray(clf)\n",
    "clfds = None\n",
    " \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFECV (number of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Init, fit\n",
    "rfecv = RFECV(\n",
    "    estimator=trainedforest,\n",
    "    min_features_to_select=3,\n",
    "    step=1,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"r2\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(img[ground_truth > 0], ground_truth[ground_truth > 0])\n",
    "\n",
    "print(f\"o n√∫mero de features a serem selecionadas √© de: {rfe.support_.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE (Recursive feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RFE \n",
    "rfe = RFE(estimator=trainedforest, n_features_to_select=6, step = 1)\n",
    "\n",
    "\n",
    "rfe.fit(img[ground_truth > 0], ground_truth[ground_truth > 0])\n",
    "\n",
    "for i in range(img.shape[2]):\n",
    "\tprint('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    " \n",
    "# read original image to get info for raster dimensions\n",
    "naip_fn = r'C:\\Users\\10333076966\\Documents\\Temp\\TestePCA\\CLASSIFICADOR\\BSI_NDVI_B1B2B3B4_NDWI_subset.tif'\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    " \n",
    "# rasterize test data for pixel-to-pixel comparison\n",
    "test_fn = r'C:\\Users\\10333076966\\Documents\\Temp\\IMAGENS_PARA_TESTE\\Amostras\\test_data.shp'\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    " \n",
    "truth = target_ds.GetRasterBand(1).ReadAsArray()  # truth/test data array\n",
    " \n",
    "pred_ds = gdal.Open(r'C:\\Users\\10333076966\\Documents\\Temp\\TestePCA\\classified_result_xgbost.tif')  \n",
    "pred = pred_ds.GetRasterBand(1).ReadAsArray()  # predicted data array\n",
    "idx = np.nonzero(truth) # get indices where truth/test has data values\n",
    "cm = metrics.confusion_matrix(truth[idx], pred[idx])  # create a confusion matrix at the truth/test locations\n",
    "scores = metrics.accuracy_score(truth[idx], pred[idx])\n",
    "\n",
    "# pixel accuracy\n",
    "print(cm)\n",
    "\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level 2 scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539ff20f4ee1d590eff0393f3a020850b595bcb7a8e76cc93c9854686ef3e2cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
