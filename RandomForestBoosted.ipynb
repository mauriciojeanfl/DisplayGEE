{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import exposure\n",
    "#from skimage.segmentation import quickshift, slic, watershed\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal, ogr\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import skimage.feature as feature\n",
    "import math\n",
    "from skimage import io, segmentation, color, feature\n",
    "\n",
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AMOSTRAS\n",
    "naip_fn = r'C:\\Users\\10333076966\\Documents\\GEOPROCESSAMENTO\\VIDEIRA\\TESTE_NOVO\\sub_COMPOSITE.tif'\n",
    "\n",
    "\n",
    "# SEGMENTO\n",
    "naip_seg = r'C:\\Users\\10333076966\\Documents\\GEOPROCESSAMENTO\\VIDEIRA\\TESTE_NOVO\\sub_segment_felzen_1.tif'\n",
    "\n",
    "\n",
    "Amostras = r'C:\\Users\\10333076966\\Documents\\GEOPROCESSAMENTO\\VIDEIRA\\TESTE_NOVO\\amostras_subset4.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPOSITE\n",
    "## PIXELSIZE\n",
    "pixelsize = 2\n",
    "\n",
    " \n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    "nbands = naip_ds.RasterCount\n",
    "band_data = []\n",
    "\n",
    "for i in range(1, nbands+1):\n",
    "    band = naip_ds.GetRasterBand(i).ReadAsArray()\n",
    "    #band_norm = preprocessing.normalize(band)\n",
    "\n",
    "    #out_put = ((band-band.min())/ (band.max()-band.min()))\n",
    "\n",
    "    #scaler = MinMaxScaler()\n",
    "    #bands = scaler.fit_transform(band)\n",
    "    band_data.append(band)\n",
    "    \n",
    "    #band_data.append(out_put)\n",
    "\n",
    "    #band_data.append(band)\n",
    "\n",
    "band_data = np.dstack(band_data)\n",
    "p2, p98 = np.percentile(band_data, (2,98))\n",
    "\n",
    "#img = band_data\n",
    "img = exposure.rescale_intensity(band_data, in_range=(p2, p98))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT SEGMENT\n",
    "\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_seg)\n",
    "segments = naip_ds.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_features(segment_pixels):\n",
    "    features = []\n",
    "    npixels, nbands = segment_pixels.shape\n",
    "    for b in range(nbands):\n",
    "        stats = scipy.stats.describe(segment_pixels[:, b])\n",
    "        band_stats = list(stats.minmax) + list(stats)[2:]\n",
    "        if npixels == 1:\n",
    "            # in this case the variance = nan, change it 0.0\n",
    "            band_stats[3] = 0.0\n",
    "        features += band_stats\n",
    "    return features\n",
    "\n",
    "def segment_sizes(segment_pixels):\n",
    "    \n",
    "    features2 = []\n",
    "\n",
    "    \n",
    "    area = (segment_img.shape[0]*segment_img.shape[1])*pixelsize^2\n",
    "    length = ((segment_img.shape[0]*segment_img.shape[1] * 4)*pixelsize)\n",
    "    compactness = 4*math.pi*area/(length^2)\n",
    "\n",
    "\n",
    "    size_stats = [area,length,compactness]\n",
    "\n",
    "    return size_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids = np.unique(segments)\n",
    "objects = []\n",
    "object_ids = []\n",
    "for id in segment_ids:\n",
    "    object3 = []\n",
    "\n",
    "    segment_mask = segments == id\n",
    "\n",
    "    # # Get the bounding box of the segment\n",
    "    coords = np.argwhere(segment_mask)\n",
    "    y0, x0 = coords.min(axis=0)\n",
    "    y1, x1 = coords.max(axis=0) + 1\n",
    "    # # Extract the corresponding portion of the original image\n",
    "    segment_img = img[y0:y1, x0:x1, :]\n",
    "\n",
    "    # grayimage__ = rgb2gray(segment_img[:, :, 1:4]).astype(np.uint16)\n",
    "    # co_mat = feature.greycomatrix(grayimage__, [1], [0], levels=2048)\n",
    "    # objects.append(feature.texture.graycoprops(co_mat, 'dissimilarity')[0, 0])\n",
    "    # #objects.append(feature.texture.graycoprops(co_mat, 'ASM')[0, 0])\n",
    "    # #objects.append(feature.texture.graycoprops(co_mat, 'correlation')[0, 0])\n",
    "\n",
    "    segment_pixels = img[segments == id]\n",
    "\n",
    "    object_features = segment_features(segment_pixels)\n",
    "    object_features_2 = segment_sizes(segment_pixels)\n",
    "    object3 = object_features+object_features_2\n",
    "    objects.append(object3)\n",
    "\n",
    "    object_ids.append(id)\n",
    "    \n",
    "objects = np.where(np.isnan(objects), 0, objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os labels serão: \n",
      "               label  id\n",
      "0            nativa   1\n",
      "1  campoagricultura   2\n",
      "2       soloexposto   3\n",
      "3              agua   4\n",
      "4         eucalipto   5\n"
     ]
    }
   ],
   "source": [
    "# read shapefile to geopandas geodataframe\n",
    "gdf = gpd.read_file(Amostras)\n",
    "# get names of land cover classes/labels\n",
    "class_names = gdf['label'].unique()\n",
    "# create a unique id (integer) for each land cover class/label\n",
    "class_ids = np.arange(class_names.size) + 1\n",
    "# create a pandas data frame of the labels and ids and save to csv\n",
    "df = pd.DataFrame({'label': class_names, 'id': class_ids})\n",
    "#df.to_csv(r'C:\\Users\\10333076966\\Desktop\\Landsat\\RESULTADOS\\1991_L5\\Amostras_1991\\class_lookup.csv')\n",
    "# add a new column to geodatafame with the id for each class/label\n",
    "gdf['id'] = gdf['label'].map(dict(zip(class_names, class_ids)))\n",
    " \n",
    "# split the truth data into training and test data sets and save each to a new shapefile\n",
    "gdf_train = gdf.sample(frac=0.8)  # 80% of observations assigned to training data (30% to test data)\n",
    "gdf_test = gdf.drop(gdf_train.index)\n",
    "# save training and test data to shapefiles\n",
    "gdf_train.to_file(os.path.join(os.path.dirname(naip_fn), 'train_data.shp'))\n",
    "gdf_test.to_file(os.path.join(os.path.dirname(naip_fn), 'test_data.shp'))\n",
    "print(f'os labels serão: \\n {df}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = os.path.join(os.path.dirname(naip_fn), 'train_data.shp')\n",
    "train_ds = ogr.Open(train_fn)\n",
    "lyr = train_ds.GetLayer()\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "# retrieve the rasterized data and print basic stats\n",
    "data = target_ds.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterized observation (truth, training and test) data\n",
    "ground_truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# get unique values (0 is the background, or no data, value so it is not included) for each land cover type\n",
    "\n",
    "classes = np.unique(ground_truth)[1:]\n",
    "\n",
    "# for each class (land cover type) record the associated segment IDs\n",
    "segments_per_class = {}\n",
    "for klass in classes:\n",
    "    segments_of_class = segments[ground_truth == klass]\n",
    "    segments_per_class[klass] = set(segments_of_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.copy(segments)\n",
    "threshold = train_img.max() + 1  # make the threshold value greater than any land cover class value\n",
    "\n",
    "# all pixels in training segments assigned value greater than threshold\n",
    "for klass in classes:\n",
    "    class_label = threshold + klass\n",
    "    for segment_id in segments_per_class[klass]:\n",
    "        train_img[train_img == segment_id] = class_label\n",
    " \n",
    "# training segments receive land cover class value, all other segments 0\n",
    "train_img[train_img <= threshold] = 0\n",
    "train_img[train_img > threshold] -= threshold\n",
    "\n",
    "\n",
    "# create objects and labels for training data\n",
    "training_objects = []\n",
    "training_labels = []\n",
    "for klass in classes:\n",
    "    class_train_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class[klass]]\n",
    "    training_labels += [klass] * len(class_train_object)\n",
    "    training_objects += class_train_object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o maximo de treino labels é: 4\n",
      "o minimo de treino labels é: 0\n",
      "o maximo de teste labels é: 4\n",
      "o minimo de teste labels é: 0\n"
     ]
    }
   ],
   "source": [
    "#testing data\n",
    "\n",
    "test_fn = os.path.join(os.path.dirname(naip_fn), 'test_data.shp')\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_test = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_test.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_test.SetProjection(naip_ds.GetProjection())\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_test, [1], lyr, options=options)\n",
    "# retrieve the rasterized data and print basic stats\n",
    "\n",
    "\n",
    "# rasterized observation (truth, training and test) data\n",
    "ground_truth_test = target_test.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# get unique values (0 is the background, or no data, value so it is not included) for each land cover type\n",
    "classes_test = np.unique(ground_truth_test)[1:]\n",
    "\n",
    "# for each class (land cover type) record the associated segment IDs\n",
    "segments_per_class_test = {}\n",
    "for klass_test in classes_test:\n",
    "    segments_of_class_test = segments[ground_truth_test == klass_test]\n",
    "    segments_per_class_test[klass_test] = set(segments_of_class_test)\n",
    " \n",
    "\n",
    "test_image = np.copy(segments)\n",
    "threshold = test_image.max() + 1  # make the threshold value greater than any land cover class value\n",
    "\n",
    "# all pixels in training segments assigned value greater than threshold\n",
    "for klass_test in classes_test:\n",
    "    class_label = threshold + klass_test\n",
    "    for segment_id in segments_per_class_test[klass_test]:\n",
    "        test_image[test_image == segment_id] = class_label\n",
    " \n",
    "# training segments receive land cover class value, all other segments 0\n",
    "test_image[test_image <= threshold] = 0\n",
    "test_image[test_image > threshold] -= threshold\n",
    "\n",
    "\n",
    "# create objects and labels for TESTING data\n",
    "testing_objects = []\n",
    "testing_labels = []\n",
    "for klass_test in classes:\n",
    "    class_test_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class_test[klass_test]]\n",
    "    testing_labels += [klass_test] * len(class_test_object)\n",
    "    testing_objects += class_test_object\n",
    " \n",
    "\n",
    "testing_labels = np.asarray(testing_labels, dtype=np.int32)\n",
    "training_labels = np.asarray(training_labels, dtype=np.int32)\n",
    "\n",
    "\n",
    "training_labels[training_labels == max(training_labels)] = 0\n",
    "testing_labels[testing_labels == max(testing_labels)] = 0\n",
    "\n",
    "\n",
    "print(f'o maximo de treino labels é: {np.max(training_labels)}' )\n",
    "print(f'o minimo de treino labels é: {np.min(training_labels)}' )\n",
    "\n",
    "\n",
    "print(f'o maximo de teste labels é: {np.max(testing_labels)}' )\n",
    "print(f'o minimo de teste labels é: {np.min(testing_labels)}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "# y_pred = classifier.predict(testing_objects)\n",
    "# cm = confusion_matrix(testing_labels, y_pred)\n",
    "# print(cm)\n",
    "# accuracy_score(testing_labels,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = cross_val_score(estimator=classifier,X=testing_objects,y=testing_labels,cv=10)\n",
    "# print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'criterion' : hp.choice('criterion', ['entropy','gini']),\n",
    "         'max_depth' : hp.quniform('max_depth',10,1200,10),\n",
    "         'max_features' : hp.choice('max_features',['auto','sqrt','log2',None]),\n",
    "         'min_samples_leaf' : hp.uniform('min_samples_leaf',0,0.5),\n",
    "         'min_samples_split' : hp.uniform('min_samples_split',0,1),\n",
    "         'n_estimators': hp.choice('n_estimators',[10,50,350,550,750,1200,1300,1500])\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier( criterion = space['criterion'],\n",
    "                                    max_depth = int(space['max_depth']),\n",
    "                                    max_features = space['max_features'],\n",
    "                                    min_samples_leaf = space['min_samples_leaf'],\n",
    "                                    min_samples_split = space['min_samples_split'],\n",
    "                                    n_estimators = space['n_estimators']\n",
    "    )\n",
    "    \n",
    "    accuracy = cross_val_score(model, training_objects,training_labels,cv=10).mean()\n",
    "    return {'loss' : -accuracy , 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space=space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals=80,\n",
    "            trials=trials\n",
    "           )\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy\n",
      "auto\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "crit = {0: 'entropy', 1: 'gini'}\n",
    "feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\n",
    "est = {0: 10, 1: 50, 2: 350, 3: 550, 4: 750, 5: 1200, 6: 1300, 7: 1500}\n",
    "\n",
    "print(crit[best['criterion']])\n",
    "print(feat[best['max_features']])\n",
    "print(est[best['n_estimators']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  1  0  0  0]\n",
      " [ 0 10  0  0  0]\n",
      " [ 0  0  9  0  0]\n",
      " [ 0  1  2 10  0]\n",
      " [ 0  0  0  0  4]]\n",
      "A acurácia global é de : 0.9149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       0.83      1.00      0.91        10\n",
      "           2       0.82      1.00      0.90         9\n",
      "           3       1.00      0.77      0.87        13\n",
      "           4       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.91        47\n",
      "   macro avg       0.93      0.94      0.93        47\n",
      "weighted avg       0.93      0.91      0.91        47\n",
      "\n",
      "O kappa é de : 0.8916\n"
     ]
    }
   ],
   "source": [
    "trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = int(best['max_depth']), \n",
    "                                       max_features = feat[best['max_features']], \n",
    "                                       min_samples_leaf = best['min_samples_leaf'], \n",
    "                                       min_samples_split = best['min_samples_split'], \n",
    "                                       n_estimators = est[best['n_estimators']]).fit(training_objects,training_labels)\n",
    "\n",
    "\n",
    "\n",
    "                                       \n",
    "predictionforest = trainedforest.predict(testing_objects)\n",
    "predicted = trainedforest.predict(objects)\n",
    "\n",
    "\n",
    "print(confusion_matrix(testing_labels,predictionforest))\n",
    "print(f'A acurácia global é de : {round(accuracy_score(testing_labels,predictionforest),4)}')\n",
    "print(classification_report(testing_labels,predictionforest))\n",
    "acc5 = accuracy_score(testing_labels,predictionforest)\n",
    "kappa = cohen_kappa_score(testing_labels, predictionforest)\n",
    "print(f'O kappa é de : {round(kappa,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_jobs=-1)  # setup random forest classifier\n",
    "# classifier.fit(training_objects, training_labels)  # fit rf classifier\n",
    "# predicted = classifier.predict(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Replace segments with the predicted classes\n",
    "clf = np.copy(segments)\n",
    "\n",
    "mask = np.isin(clf, segment_ids)\n",
    "clf[mask] = predicted[np.searchsorted(segment_ids, clf[mask])]\n",
    "\n",
    "mask = np.sum(np.abs(img), axis=2)\n",
    "mask[mask > 0.0] = 1.0\n",
    "mask[mask == 0.0] = -1.0\n",
    "clf = np.multiply(clf, mask)\n",
    "clf[clf < 0] = -9999.0\n",
    "\n",
    "# Save the result to raster\n",
    "clfds = driverTiff.Create(r'C:\\Users\\10333076966\\Documents\\GEOPROCESSAMENTO\\VIDEIRA\\TESTE_NOVO\\CLASSIFIED2.tif', naip_ds.RasterXSize, naip_ds.RasterYSize,\n",
    "                          1, gdal.GDT_Float32)\n",
    "clfds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "clfds.SetProjection(naip_ds.GetProjection())\n",
    "clfds.GetRasterBand(1).SetNoDataValue(-9999.0)\n",
    "clfds.GetRasterBand(1).WriteArray(clf)\n",
    "clfds = None\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFECV (number of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Init, fit\n",
    "rfecv = RFECV(\n",
    "    estimator=trainedforest,\n",
    "    min_features_to_select=3,\n",
    "    step=1,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"r2\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(img[ground_truth > 0], ground_truth[ground_truth > 0])\n",
    "\n",
    "print(f\"o número de features a serem selecionadas é de: {rfe.support_.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE (Recursive feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RFE \n",
    "rfe = RFE(estimator=trainedforest, n_features_to_select=6, step = 1)\n",
    "\n",
    "\n",
    "rfe.fit(img[ground_truth > 0], ground_truth[ground_truth > 0])\n",
    "\n",
    "for i in range(img.shape[2]):\n",
    "\tprint('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    " \n",
    "# read original image to get info for raster dimensions\n",
    "naip_fn = r'C:\\Users\\10333076966\\Documents\\Temp\\TestePCA\\CLASSIFICADOR\\BSI_NDVI_B1B2B3B4_NDWI_subset.tif'\n",
    "driverTiff = gdal.GetDriverByName('GTiff')\n",
    "naip_ds = gdal.Open(naip_fn)\n",
    " \n",
    "# rasterize test data for pixel-to-pixel comparison\n",
    "test_fn = r'C:\\Users\\10333076966\\Documents\\Temp\\IMAGENS_PARA_TESTE\\Amostras\\test_data.shp'\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', naip_ds.RasterXSize, naip_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(naip_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(naip_ds.GetProjection())\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    " \n",
    "truth = target_ds.GetRasterBand(1).ReadAsArray()  # truth/test data array\n",
    " \n",
    "pred_ds = gdal.Open(r'C:\\Users\\10333076966\\Documents\\Temp\\TestePCA\\classified_result_xgbost.tif')  \n",
    "pred = pred_ds.GetRasterBand(1).ReadAsArray()  # predicted data array\n",
    "idx = np.nonzero(truth) # get indices where truth/test has data values\n",
    "cm = metrics.confusion_matrix(truth[idx], pred[idx])  # create a confusion matrix at the truth/test locations\n",
    "scores = metrics.accuracy_score(truth[idx], pred[idx])\n",
    "\n",
    "# pixel accuracy\n",
    "print(cm)\n",
    "\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level 2 scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539ff20f4ee1d590eff0393f3a020850b595bcb7a8e76cc93c9854686ef3e2cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
